---
title: "Using Probability and Simulation to Solve Business Problems"
output: learnr::tutorial
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
library(tidyverse)
knitr::opts_chunk$set(echo = T)
sims <- 100000
```


## Introduction

The purpose of this tutorial is to give you the knowledge, skills and example code  needed to complete the case for this module. The focus will be on probability and simulation.

Probability is generally discussed theoretically, with  definitions and rules. The examples used to teach probability likewise tend to be theoretical and abstract, relying on coins, cards, dice, balls and urns. (Urns? When was the last time outside of a class on probability that you encountered an urn?) 

The examples in this tutorial will be practical, illustrating how probability and simulation can be used to model events and solve business problems.
 
## Probability

Curiously, probability is not defined very clearly in either the reading or the DataCamp course assigned for this module. So, here goes:

For our purposes, **probability is relative frequency**: how many times does an event occur relative to how many times it could occur?  What is the probability of rain in July? Count the number of  Julys with rain and divide by the total number of Julys in the sample.  As such, **probability is always  expressed as a proportion between 0 and 1**, with a larger number indicating greater likelihood that the event will happen. Furthermore, the **probability  of mutually exclusive events in the sample space always sums to 1**.  Mutually exclusive just means that one or the other of these events must happen, but not both together.  Either it rains or it doesn't. In this situation we say that the **complement** of event $A$, rain, is event $A^c$, no rain:

![](images/complement.png)


In sum:

- $P(A) \in [0,1]$
- $A^c$ is $P(A^c) = 1 - P(A)$
- $A \:\text{or}\: B$ is $P(A \:\text{or}\: B) = P(A) + P(B)$, if A and B are mutually exclusive.

Discussions of probability can quickly become technical.  It is worthwhile remembering that  probability just uses historical frequencies to express the likeliness of an event.  In this respect, you can think of it as a rate of occurrence. Of course, sometimes historical data is not available, in which case the assignment of probability to an event becomes somewhat subjective. 

It is  also worthwhile remembering that even people trained in probability are prone to [cognitive bias](https://en.wikipedia.org/wiki/Cognitive_bias) in  reasoning about probability and making decisions, so formal analysis is often warranted.


### Probability of A or B

If A and B are mutually exclusive events, then $P(A \:\text{or}\: B) = P(A) + P(B)$. 
![](images/disjoint.png)

What if the events are not mutually exclusive?  We need to make the rule more general: $P(A \:\text{or}\: B) = P(A) + P(B) - P(A \:\text{and}\: B)$.

![](images/not disjoint.png)

Example:  rain in July and an eclipse in July are not mutually exclusive---they can both happen---so for the total probability to sum to 1 we must subtract the overlap: the proportion of times they happen together.

### Probability of A and B

![](images/multiplication.png)

$P(A \:\text{and}\: B)= P(A) * P(B)$ if A and B are independent events. Independence means that the occurrence of A  does not influence the occurrence of B.

### Conditional Probability

$P(A|B)$, the notation for conditional probability, is read as: "the probability that A  occurs given that event B occurs." 

$P(A|B) = \frac{P(A \:\text{and}\: B)}{P(B)}$. 

This formulation looks intimidating but  captures a commonsense notion. Let's say that you are calculating the probability of loan default and want to examine the prevalence  of default by gender. To get the **unconditional** probability of default you would simply count the number of defaults and divide by the number of customers. To get the **conditional** probability of default for men you would simply count the number of defaults among men and divide by the number of men. In calculating conditional probability, the condition---in this case, being male---becomes the denominator.

### Random Variables and Probability Distributions

A **random variable** is a rule that assigns a number to every outcome in the sample space of an experiment. A **discrete random variable** is a random variable that can take on a countable number of distinct values. A **continuous random variable** is a random variable that can take on any value within a range of infinite (uncountable) values.

Some values are more likely than others within the range of a random variable's possible values. A random variable therefore has an associated **probability distribution** based on the probability of its possible values.

R has a variety of functions for working with random variables and probability distributions.  We will focus on the functions used to:

1. Simulate particular outcomes of a random variable, known as random variates. This is equivalent to taking random draws---samples---from the probability distribution defined by  the random variable. We can use these samples to do approximate probability calculations.

2. Do exact probability calculations using the theoretical properties of the distribution.

## Functions to Sample Random Numbers

- `rnorm(n, mean, sd)` draws from a [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution) with parameters $\mu$ and $\sigma$, represented as $N(\mu, \sigma)$
- `rbinom(n, size, prob)` draws from a [binomial distribution](https://en.wikipedia.org/wiki/Binomial_distribution) with parameters $n$ (the size of the trial) and $p$, represented as $Binomial(n, p)$.
- `sample(x, size, replace, prob)` draws from any categorical distribution we wish to define.
<!-- - `rbeta(n, a, b)` simulates from $Beta(\alpha, \beta)$ -->

### rnorm()

The normal or Gaussian distribution is useful because it describes a wide range of phenomena, including the distribution of sample means, a fact that supplies the basis of classical statistical inference. The normal distribution is completely described by the parameters $\mu$ and $\sigma$.  The *standard* normal distribution is defined by $\mu = 0$ and $\sigma = 1$ and represented as $N(0,1)$. 

Here is code to draw 10 observations from $N(0,1)$:

```{r random, exercise = T}
# Simulate from N(0,1)
rnorm(10, mean = 0, sd = 1)
```

These are random draws, so every time this code is run the numbers will change.  Try it. In order to take random draws that do not change---and to use numbers that are therefore suitable for reproducible data analysis---we must set a random seed using the `set.seed()` function before running any random process.  The argument to `set.seed()` is a number.  It doesn't matter which number. As a general rule, **always set the seed.**

```{r random1, exercise = T}
# Draw with set.seed(123)
set.seed(123)
rnorm(10, mean = 0, sd = 1)

# Identical random draw
set.seed(123)
rnorm(10, mean = 0, sd = 1)
```



### rbinom()

The binomial distribution is important because it forms the basis of many other distributions (including the normal distribution).  It is completely defined by the size and probability parameters.  Remember that the `size` argument indicates the number of trials (the size of the experiment), not the number of draws.  

Here is code to draw 10 observations from, for example, a $Binomial(10, .5)$:

```{r random2, exercise = T}
# Draw from Binomial(10, .5)
set.seed(123)
rbinom(n = 10, size = 10, prob = .5)
```

<!-- ### Density -->

<!-- The density functions calculate the probability density function (PDF) in the case of continuous distributions and the probability mass function (PMF) in the case of discrete distributions. -->

<!-- - `dbinom(x, size, prob)` -->
<!-- - `dnorm(x, mean, sd)` -->
<!-- - `dbeta(x, a, b)` -->

<!-- For example, `dnorm()` calculates the height of the density curve at a particular point (actually, an interval), as defined by the normal density function:  $f(x|\mu,\sigma) = \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$.  The density around -1 in $N(0,1)$ is: -->

<!-- ```{r echo = T} -->
<!-- dnorm(x = -1, mean = 0, sd = 1) -->
<!-- ``` -->

<!-- This density calculation corresponds to the y-axis value in the density plot at -1: -->

<!-- ```{r} -->
<!-- data.frame(x = rnorm(10000, mean = 0, sd = 1)) %>%  -->
<!-- ggplot(aes(x)) + -->
<!--   geom_density()+ -->
<!--   labs(title = "Random draws from N(0, 1)") + -->
<!--   theme_minimal() -->
<!-- ``` -->

<!-- And the PMF for 1 from $Binomial(1, .5)$ is .5 because with p = .5 half the density will be on 1 and half on 0:  -->

<!-- ```{r echo = T} -->
<!-- dbinom(x = 1, size = 1, prob = .5) -->
<!-- ``` -->

<!-- What happens if we change the probability argument to .6? -->

<!-- ```{r echo = T} -->
<!-- dbinom(x = 1, size = 1, prob = .6) -->

<!-- ``` -->

### sample()

The `sample()` function can be used to draw from any categorical distribution.  As an introduction, consider that these two code snippets are exactly equivalent:

```{r random3, exercise =T}
# Draw from Binomial(1, .5) with rbinom()
rbinom(n = 10, size = 1, prob = .5)

# Draw from Binomial(1, .5) with sample()
sample(x = c(0, 1), size = 10, replace = T, prob = c(.5, .5))

```

In contrast to `rbinom()`, the `size` argument in `sample()` represents the *number* of random variates generated. Additionally:

- `x` is the source vector from which to take random draws.
- `prob` defines the probability of those draws.  
- `replace` indicates whether each draw should be returned to the source vector before the next draw. If `size` is larger than the number of items in `x`, then `replace` must be `TRUE`.

Here is code to take 50 random draws from four categories---A, B, C, D---with probability of .1, .2, .3, .4, respectively. 

```{r random4, exercise=T}
# Sample from 4 categories with defined probability
set.seed(123)
sample(c("A", "B", "C", "D"), 
       size = 50, 
       replace = T, 
       prob = c(.1, .2, .3, .4)) 

```

With `size = 50` the proportions should be close to what we defined (but would, of course, become more exact with larger n):

```{r random5, exercise=T}
# Summarize proportions in random sample
set.seed(123)
sample(c("A", "B", "C", "D"), 
       size = 20, 
       replace = T, 
       prob = c(.1, .2, .3, .4)) %>%
  table %>% 
  prop.table
```

## Functions to Calculate Probability

- `pnorm(q, mean, sd)`
- `pbinom(q, size, prob)`

The probability functions calculate the cumulative distribution function (CDF), which is defined as the probability that for any given number $q$ the observed value of the random variable will be at most $q$. In the case of a continuous distribution, the CDF gives the area under the probability density curve up to $q$. Hence `pnorm()` returns the proportion of total probability that occurs in the interval $[-\infty, q]$

### pnorm()

Take $N(0,1)$ as an example.  What is the cumulative probability (area under the curve) up to 0, indicated by the shaded portion in the plot below?  Since the mean of the distribution is 0, and the entire area under the curve is 1, the cumulative probability to the left of 0 should be .5:

```{r echo = F, warnings = F, message = F}
probs <- data.frame(q = seq(-3, 3, length=1000))
probs$density <- dnorm(probs$q, 0, 1)
plt <- ggplot(probs, aes(q, density)) +
    geom_line()+
  geom_area(aes(ifelse(q<0 , q, NA), density), fill = 2, alpha = .5, na.rm=T)+
  theme_minimal()
suppressWarnings(print(plt))
```


Here is the code for doing this calculation:

```{r}
# Calculate CDF [neg inf, 0]
pnorm(q = 0, mean = 0, sd = 1)
```



What is the area under the density curve for $N(0,1)$ between -1 and 1, corresponding to the shaded area of this plot?

```{r echo = F, warnings = F, message = F}
probs <- data.frame(q = seq(-3, 3, length=1000))
probs$density <- dnorm(probs$q, 0, 1)
plt <- ggplot(probs, aes(q, density)) +
    geom_line()+
  geom_area(aes(ifelse(q< 1 & q > -1 , q, NA), density), fill = 2, alpha = .5)+
  theme_minimal()
suppressWarnings(print(plt))
```

We subtract the cumulative probability up to $q = -1$ from the larger cumulative probability up to $q = 1$. Here is the calculation using `pnorm()`.  

```{r calc, exercise = T}
# Calculate CDF [-1, 1]
pnorm(q = 1, mean = 0, sd = 1) - 
  pnorm(q = -1, mean = 0, sd = 1)
```

Incidentally, we can approximate this exact calculation with simulation using `rnorm()`:

```{r calc_alt, exercise = T}
# Simulate CDF [-1, 1]
set.seed(123)
sims <- 100000
draws <- rnorm(n = sims, mean = 0, sd = 1) 
mean(draws <= 1) - mean(draws <= -1)
```

Notice that I have defined the number of simulations---`sims <- 100000`---at the top of the code and will use `sims` in the reminder of the tutorial to specify the number of random draws in each simulation.  Why? It is best to avoid hard-coding such numbers: in case you want to change the number you only have to change it once, where you initially defined it.

### pbinom()

We can, of course, calculate a CDF for a discrete distribution also, such as $Binomial(1,.5)$. How much of the probability in this distribution is associated with 0 rather than 1?  Another way to put the question:  what is the relative frequency of 0's in the distribution as opposed to 1's?  This should be .5 by definition.

```{r}
# Calculate CDF for [0, 0]
pbinom(q = 0, size = 1, prob = .5)
```

## Business Applications

In the reading you were introduced to **discrete and continuous probability** distributions and their properties.  The simulation techniques  introduced in the DataCamp course provide an extremely powerful--- and, I think, intuitive--- way to solve probability problems that relies more on computation than theory.  What follows are some examples demonstrating the use of these techniques. 

### Discrete Probability Examples

#### Oil discovery

Bonanza Oil is drilling for oil at three promising sites. According to geological test, the probabilities of finding oil at these three sites are .7, .85, and .8 respectively. The presence of oil at any one of the sites is presumed to be independent of the presence of oil at any of the other sites. 

(a)  What is the probability of finding oil at all three of the sites? 

$P(\text{oil}) = P(A \:\text{and}\: B \:\text{and}\: C)$ = .7 x .85 x .8 

```{r discrete, exercise=TRUE}
# Calculate p(oil) using probability rules
.7 * .85 * .8

```

(b)  What is the probability of not finding oil at any of the three sites?

Finding oil at all three sites and finding oil at none of the sites are mutually exclusive events: they cannot both happen. Therefore, if we know the probability of one event we can derive the probability of the other event. $P(\text{no oil}) = 1 - P(\text{oil})$.

Let's say you have forgotten all of the probability rules. Could you simulate this result? Yes. The value of simulation is that it  offers a general solution to these sorts of problems. The challenge is to correctly identify the type of event being simulated. In this case the event is the discovery of oil at a particular site, which is binary: oil is either discovered or not discovered. So we will model discovery as a binomial random variable of size 1 with the stipulated probabilities and use the `rbinom()` function to draw from each distribution.

```{r sim1, exercise=TRUE}
# Simulate oil discovery at 3 sites
set.seed(123)
site1 <- rbinom(n = sims, size = 1, prob = .7)
site2 <- rbinom(n = sims, size = 1, prob = .85)
site3 <- rbinom(n = sims, size = 1, prob = .8)

# Estimate p(oil)
mean(site1 & site2 & site3)

# Estimate p(no oil)
mean(!(site1 & site2 & site3))

```

#### Automobile manufacturing

The paint department in a Ford factory  applies two processes when painting cars: painting and polishing.  The painting process is defective 20% of the time, while the polishing process is defective 10% of the time. Each car first goes through the painting and then through the polishing process. Each car is inspected after it has completed the two processes. If either the painting or the polishing is defective, the car is returned to a special station for rework, where the two processes are applied once again. Rework at the special station is 100% reliable (although it is very expensive).

Let X be the number of cars in a group of 1000 cars that have painting defects. Let Y be the number of cars in a group of 1000 cars that have polishing defects. Both X and Y are binomial random variables: X = $Binomial(1000, .1)$ and Y = $Binomial(1000, .2)$.

(a)  What is the probability that a car is returned to the special station for rework?

$P(X \:\text{or}\: Y) = P(X) + P(Y) - P(X \:\text{and}\: Y)$ = .1 + .2 - .1 x .2

```{r sim2, exercise = T}
# Calculate p(X or Y) using probability rules
.1 + .2 - .1 * .2 
```

Approximate this result with simulation:

```{r sim3, exercise = T}
# Simulate X and Y
set.seed(123)
X <- rbinom(n = sims, size = 1, prob = .1)
Y <- rbinom(n = sims, size = 1, prob = .2)

# Estimate p(X or Y)
mean(X | Y) %>% round(2)

```

```{r echo=F}
set.seed(123)
X <- rbinom(n = sims, size = 1, prob = .1)
Y <- rbinom(n = sims, size = 1, prob = .2)
mean(X | Y) %>% round(2)

```

(b)  In a batch of 1000 cars, what is the expected number of cars that will be returned for rework?

The mean of a binomial distribution is $n*p$:  1000 x .28 = 280.

We get the same result using simulation:

```{r sim4, exercise = T}
# Estimate number of cars with defects
(sum(X | Y)/100) %>%  round() 
```


(c)  In a batch of 1000 cars, what is the probability that the number of returned cars is less than or equal to 250?

```{r sim5, exercise = T} 
# Estimate p(returned cars) <= 250 from simulation
set.seed(123)
sum(rbinom(n = sims, size = 1000, prob = .28) <= 250)/sims

# Calculate directly from theoretical CDF
pbinom(q = 250, size = 1000, prob = .28)
```

<!-- (d) In a batch of 1000 cars, what is the probability that the total number of defects, X plus Y, is less than or equal to 300? -->

<!-- ```{r sim6, exercise = T}  -->

<!-- (rbinom(n, 1000, .1) + rbinom(n, 1000, .2) <= 300) %>%  -->
<!--   mean -->

<!-- ``` -->

<!-- We could also use the normal approximation to the binomial distribution to verify this result, with $\mu = size*p$ and $\sigma = \sqrt{size*p*(1-p)}$.  $E[X + Y]$ = $E[X] + E[Y]$, so $\mu = 1000 * .1 + 1000 * .2 = 300$. $Var[X + Y]$ = $Var[X] + Var[Y}$ (and $\sigma = \sqrt{Var}$), so $\sigma = \sqrt{1000*0.2*0.8 + 1000*0.1*0.9}$ = 15.8. -->

<!-- ```{r} -->
<!-- mean(rnorm(10000, 300, 15.8) <= 300) # or -->

<!-- pnorm(300, 300, 15.8) -->
<!-- ``` -->


<!-- ## Defective Lamps -->

<!-- A hardware store has received two shipments of halogen lamps. The first shipment contains 100 lamps, 4% of which are defective. The second shipment contains 50 lamps, 6% of which are defective.  Suppose a customer picks a lamp (at random) off of the shelf and purchases it, and later discovers that the lamp is defective. Is that defective lamp more likely to come from the first shipment or from the second shipment? -->

<!-- There are 4 defective lamps in the first shipment and .06 x 50 = 3 defective lamps in the second shipment.  The question is whether P(shipment 1 | defective) is greater than or less than P(shipment 2 | defective).  The denominator in calculating conditional probability is therefore the number of defective lamps in the two shipments.  P(shipment 1 and defective)/ P(defective) = 4/7 versus P(shipment 2 and defective)/ P(defective) = 3/7.  So the lamp likely came from shipment 1.   -->

<!-- A simulation is not necessary. -->


<!-- ## Cleanser Sales -->

<!-- The weekly sales of a brand name pitching cleanser at a supermarket is believed to be normally distributed with the meaning of 2550 bottles and a standard deviation of 415 bottles. The store manager places an order at the beginning of each week for the cleanser. She would like to carry enough bottles of the cleanser so that the probability of stocking out (not having enough bottles of the cleanser) is only 2.5% How many bottles should she order each week? -->

<!-- ```{r} -->
<!-- rnorm(100000, 2550, 415) %>%  -->
<!--     quantile(probs = .025) -->

<!-- # Or, exactly -->

<!-- qnorm(.025, 2550, 415) -->
<!-- ``` -->

### Continuous Probability Examples

#### Air pumps

KLEERCO supplies under-hood emission-control air pumps to the automotive industry.  the pump is vacuum-powered and works when the engine is operating, cleaning the exhaust by pumping extra oxygen into the exhaust system. If a pump fails before the vehicle in which it is installed has traveled 50,000 miles, Federal emission regulations require that it be replaced at no cost to the vehicle owner. The company's current air pump lasts an average of 61,000 miles, with a standard deviation of 9000 miles. The number of miles a pump operates before becoming ineffective has been found to obey a normal distribution.

(a) For the current pump design, what percentage of the company's pumps will have to be replaced at no charge to the vehicle owner?

First, we will simulate an answer using `rnorm()`.

```{r calc4, exercise =T}
# Simulate replacements
set.seed(123)
pump_lifespan <- rnorm(n = sims, mean = 61000, sd = 9000)

# Estimate number of replacements
(pump_lifespan < 50000) %>% 
  mean * 100

```

```{r echo=F}
set.seed(123)
pump_lifespan <- rnorm(n = sims, mean = 61000, sd = 9000)


```

And here is an exact calculation using `pnorm()`:

```{r calc5, exercise=T}
# Calculate replacements using theoretical CDF
pnorm(q = 50000, mean = 61000, sd = 9000) * 100

```

(b)  What percentage of the company's pumps will fail at exactly 50,000 miles?

```{r  calc6, exercise=T}
# Estimate p(failure == 50000)
(pump_lifespan == 50000) %>% 
  mean * 100

```

This may seem like an odd result but, technically, the probability density of any single point in a continuous probability distribution is 0.

(c)  What percentage of the company's pumps will fail at mileage between 42,000 and 57,000?

```{r  calc7, exercise=T}
# Estimate p(replacement) between 42k and 57k
(pump_lifespan > 42000 & pump_lifespan < 57000) %>% 
  mean * 100 # or

# Calculate directly
(pnorm(q = 57000, mean = 61000, sd = 9000) -
  pnorm(q = 42000, mean = 61000, sd = 9000)) *100

```

## Simulation Modeling

A simulation model is a computer representation of a problem that involves random variables and their associated probability distributions.  The chief advantage of a simulation model is that it can forecast the consequences of various management decisions before such decisions must be made. Simulation models are used in a wide variety of management settings:

- Modeling of manufacturing operations.
- Modeling of service operations where queues form (such as in banking, passenger air travel, food services, etc.).
- Modeling of investment alternatives.
- Analysis and pricing of sophisticated financial instruments. 

A simulation model is an extremely useful tool to help the manager make difficult decisions in an environment of uncertainty.

### Restaurant Example

Sanjay Thomas, a second year MBA student, has been offered a job in a consulting firm in Mumbai with an excellent starting salary of \$80,000 per year. He has also considered opening a restaurant, which appeals to him because it would involve business ownership and less travel.  He would also not have to relocate his family. To compare the two opportunities Sanjay must estimate potential restaurant profit. He feels he needs to make at least $5000 per month in order to maintain a reasonable lifestyle and pay off his school loans.

Sanjay estimates the following fixed costs for the restaurant:

```{r echo = F}

(fixed <- data.frame(category = c("Rent", "Leased Equipment", "Utilities", "Insurance", "Loan Repayment", "Advertising/Promotion","Miscellaneous"),
           cost = c(3000, 275, 265, 155, 125, 100, 75)))

```

The total is \$3995.

After some market research, Sanjay realizes that the price that could be charged for a meal would likely fluctuate with the strength of the economy.  He estimates the following range of meal prices, with the probability of market strength based on his historical research.

```{r echo = F}
(market <- data.frame(market = c("very helpful","healthy","not so healthy","unhealthy"),
                      meal_price = c(20,18.50, 16.50, 15),
                      probability = c(.25, .35, .3, .1)))
```

Sanjay estimates that the restaurant would have seating capacity for 50 and that the number of meals sold per month would be normally distributed with $\mu$ = 3000 and $\sigma$ = 1000. Food costs will likely be \$11 per meal.

He thinks the number of kitchen staff---chef, wait staff, kitchen staff---would probably vary between 5 and 8 with monthly labor costs in the range of \$5000 to \$7000.  The actual cost could be any value in this range with equal probability.

- How much profit can Sanjay  expect to make from the restaurant per month?  
- How much would monthly profit likely vary based on fluctuations in demand, economic strength and labor costs?  
- What choice should he make?

### Simulation Steps

1. Set up an empty dataset.
2. Simulate values based on random variables.
3. Analyze the simulation.

**Set up an empty dataset**. Many columns will start out empty, to be filled with simulated values based on the above random variables. 1000 is a good number of simulations to start with.

```{r frame, exercise =T}
# Set up empty data frame
n <- 1000
sim <- data.frame(months = 1:n,
                     fixed_costs = 3995,
                     meals_sold = NA,
                     price_per_meal = NA,
                     profit_per_meal = NA,
                     labor_costs = NA,
                     profit = NA)
head(sim)


```


```{r echo=F}
# Define empty data.frame
n <- 1000
sim <- data.frame(months = 1:n,
                     fixed_costs = 3995,
                     meals_sold = NA,
                     price_per_meal = NA,
                     profit_per_meal = NA,
                     labor_costs = NA,
                     profit = NA)



```

**Simulate values based on random variables** and fill in calculated fields. We already know fixed costs.  `Profit per meal` will be `price per meal` minus \$11. `Profit` will be a calculated field. `Profit` = `meals sold` x `profit per meal` - `labor costs` - `fixed costs`.

The other columns will contain simulated values. 

Note that we will simulate labor costs from the [uniform distribution](https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)) using the `runif()` function.  A uniform distribution is continuous over a fixed interval defined by two parameters, minimum and maximum, and each observation has the same probability of being  drawn.

```{r simframe, exercise=T}
# Fill in data frame with simulated values
set.seed(123)
sim <- sim %>% 
  mutate(meals_sold = rnorm(n = n, mean = 3000, sd = 1000),
         price_per_meal = sample(x = c(20, 18.50, 16.50, 15),
                                 size = n,
                                 replace = T,
                                 prob = c(.25, .35, .3, .1)),
         profit_per_meal = price_per_meal - 11,
         labor_costs = runif(n = n, min = 5000, max = 7000),
         profit = meals_sold * profit_per_meal - labor_costs - fixed_costs)
  
head(sim)
```

**Analyze the simulation.**   The estimate of profit now contains the variability in meals_sold, price_per_meal and labor_costs. Sanjay needs an income of \$5000. How does our simulated profit compare to this number? We can plot the distribution, and calculate its expected value and standard deviation, as well as the probability that  Sanjay's income from the restaurant would be less than \$5000---an important consideration.

```{r plot, exercise = T}
# Visualize distribution of profit
ggplot(sim, aes(profit)) +
  geom_density() +
  geom_vline(xintercept = 5000, col = 2) +
  geom_vline(xintercept = 0, col = 2, lty = 2) +
  theme_minimal()+
  labs(title = "Distribution of profit")
```

This looks promising. Certainly, the average of this  simulated profit distribution appears to be greater than \$5000.  However, a portion of the distribution is below \$5000, and, indeed, even below zero. 

This is one of the virtues of a simulation study: it provides a sense of the full range of possible--- and probable--- outcomes and makes decision-making less prone to the misleading simplifications inherent in simply calculating the mean.  This distribution shows the real risk associated with the restaurant venture, and will hopefully make Sanjay's decision-making more informed.

So what is the mean of simulated profit?  Let's examine other summary numbers at the same time.

```{r summary, exercise = T}
# Summarize profit
sim$profit %>% 
  summary
```


Average profit is over \$11,000 per month. But the minimum is actually a loss of almost \$9,000! That is quite a bit of volatility. What is the probability that Sanjay's monthly profit would be below his threshold of \$5000?

```{r summary2,exercise = T}
# Estimate p(profit < 5000)
(sim$profit < 5000) %>% 
  mean
```

What is the probability that Sanjay's monthly profit would be below \$0?

```{r summary2,exercise = T}
# Estimate p(profit < 0)
(sim$profit < 0) %>% 
  mean
```

Sanjay should therefore expect to make a monthly profit of less than \$5000 about  30% of the time, and take a loss about 10% of the time, even as his average profit is substantially above that.

Should he open the restaurant? That is his decision. But simulation modeling should help him make a  more informed decision.

<!-- An athletic footwear company, Swift Shoes, is attempting to estimate the sales that will result from a television advertisement campaign of its new running shoe. The contribution to earnings from each pair of shoes sold is $40. Suppose that the probability that a television viewer will watch the advertisement (as opposed to turn his/her attention elsewhere) is .4. Furthermore, suppose that 1% of viewers who watch the advertisement on a local television channel will buy a pair of shoes. The company can buy television advertising time (denominated in dollars/minute) in one of the time slots according to this table: -->

<!-- ```{r, echo = F} -->
<!-- data.frame(Slot = c("Morning", "Afternoon", "Prime Time", "Late Evening"), -->
<!--            Cost = c(12000, 20000, 40000, 15000), -->
<!--            Viewers = c(100000,130000, 320000, 80000)) -->
<!-- ``` -->

<!-- Suppose that the company decides to buy one minute of advertising time. Which time slot would yield the highest expected contribution to earnings? -->

<!-- .01 of the the .4 viewers who are paying attention will spend 40 dollars on a pair of shoes. The calculation of earnings per time slot would be: viewers x .01 x .4 x 40 - cost. -->

<!-- ```{r} -->
<!-- # Morning -->
<!-- 100000 * .01 * .4 * 40 - 12000 -->

<!-- # Afternoon -->
<!-- 130000 * .01 * .4 * 40 - 20000 -->

<!-- # Prime -->
<!-- 320000 * .01 * .4 * 40 - 40000 -->

<!-- # Late evening -->
<!-- 80000 * .01 * .4 * 40 - 15000 -->
<!-- ``` -->

<!-- Looks like prime time is the best. -->

<!-- Let's add an additional wrinkle to this scenario. The probability that just 40% of viewers will be paying attention to the advertisement is an estimate. In reality, that distribution of attention will fluctuate from day to day, even as the overall average remains at .4. Suppose that morning and afternoon viewers tend to vary a lot in their attention---sometimes they are focused, sometimes not---depending on the day and the news. Prime time viewers have the least variability in attention---they are always close to .4---followed by late night viewers.  In other words, attention is a random variable, the distribution of which has a different shape, or spread, in the different time slots. -->

<!-- The company's goal is to avoid the possibility of a loss in purchasing advertising while also picking the time slot with the greatest probability for positive earnings. -->

<!-- We need to replace the single number we used to estimate the probability of paying attention, .4, with a probability distribution centered at .4, in order to capture likely variation in the proportion of viewers paying attention.  For that we can use the beta distribution, which is a probability for proportions, bounded at 0 and 1.  The two parameters for the beta distribution, $\alpha$ and $\beta$, determine its shape:  the larger those parameters are, the lower the variability.  The mean of a beta distribution is defined as $\frac{\alpha}{\alpha + \beta}$.  For example, if $\alpha$ = 2 and $\beta$ = 3, then the mean is .4.  Likewise, if $\alpha$ = 4 and $\beta$ = 6, then the mean is also .4, but the range of values is narrower. -->

<!-- The following is a visualization of: -->
<!-- - beta(2, 3) for the distribution of morning attention. -->
<!-- - beta(2, 3) for the distribution of afternoon attention. -->
<!-- - beta(8, 12) for the distribution of prime time attention. -->
<!-- - beta(4, 6) for the distribution of late evening attention. -->

<!-- ```{r echo = F} -->
<!-- # Create data frame of attention probabilities -->
<!-- alpha <- 4 -->
<!-- beta <- 6 -->
<!-- sims <- 1000 -->

<!-- # Use rbeta(n, alpha, beta) for simulation with n = 1000 -->
<!-- # rep() stands for repeat.  The arguments are x, the item to be -->
<!-- # repeated, and n, the number of repeats. -->

<!-- # This data frame will have 1000 simulations for each time slot -->

<!-- probs <- data.frame(slot = c(rep("morning", sims), -->
<!--                           rep("afternoon", sims), -->
<!--                           rep("prime", sims), -->
<!--                           rep("late evening", sims)), -->
<!--                     viewers = c(rep(100000, sims), -->
<!--                                 rep(130000, sims), -->
<!--                                 rep(320000, sims), -->
<!--                                 rep(80000, sims)), -->
<!--                     cost = c(rep(12000, sims), -->
<!--                              rep(20000, sims), -->
<!--                              rep(40000, sims), -->
<!--                              rep(15000, sims)), -->
<!--                     probability_of_buying=.01, -->
<!--                     purchase_amount = 40, -->
<!--                     probability_of_attention = c(rbeta(sims, alpha/2, beta/2), -->
<!--                                  rbeta(sims, alpha/2,beta/2), -->
<!--                                  rbeta(sims, alpha*2,beta*2), -->
<!--                                  rbeta(sims, alpha,beta))) -->

<!-- # Look at data by slot -->
<!-- # probs %>% -->
<!-- #   group_by(slot) %>% -->
<!-- #   slice(1:5) -->

<!-- # Plot distributions -->
<!-- ggplot(probs, aes(probability_of_attention, col=slot))+ -->
<!--   geom_density() + -->
<!--   labs(title = "Distribution of attention by time slot")+ -->
<!--   theme_minimal() -->
<!-- ``` -->


<!-- Note that because earnings depends on attention, which we are now modeling as a random variable, earnings is itself a random variable.  Earnings will have a distribution that depends on the distribution of attention. We can use the formula we used above in calculating earnings, but replace .4 probability with our randomly fluctuating simulated probability of attention: viewers x .01 x simulated beta probability x 40 - cost. In other words, we need to fill in earnings in this table (which is displaying 5 observations from each time slot): -->

<!-- ```{r echo = F} -->
<!-- probs %>% -->
<!--   mutate(earnings = "?") %>%  -->
<!--    group_by(slot) %>% -->
<!--    slice(1:5) -->

<!-- ``` -->

<!-- After calculating earnings we can plot its distribution: -->

<!-- ```{r}   -->
<!-- # Calculate earnings -->
<!-- probs <- probs %>% -->
<!--   mutate(earnings = viewers * .01 * probability_of_attention * 40 - cost) -->

<!-- # Visualize earnings -->
<!-- ggplot(probs, aes(earnings, col = slot)) + -->
<!--   geom_density() + -->
<!--   geom_vline(xintercept = 0, col = 2, lty = 2) + -->
<!--   labs(title = "Distribution of earnings by time slot")+ -->
<!--   theme_minimal() -->

<!-- ``` -->

<!-- This analysis is revealing. Estimated earnings are sometimes negative for all the slots, even as the averages, for all but late evening, are positive. Averages can  be misleading; simulating a distribution for attention rather than using a single number provides realism.  Does the company even want to do any advertising if there is a substantial possibility of a loss?  That is a valid question. If they do want to advertise then we should pick the time slot that has not the *highest average* but the *highest probability* of positive earnings (they may be the same). We can calculate that by calculated the proportion of positive earnings per slot: -->

<!-- ```{r} -->
<!-- probs %>% -->
<!--   group_by(slot) %>% -->
<!--   summarize(`positive probability` = sum(earnings > 0) / sims) -->
<!-- ``` -->

<!-- Prime time is still the best, with .77 probability of positive earnings.  But it is useful to know for decision-making for purposes that there is a 1 - .77 probability of zero or negative earnings. -->


<!-- ## Case: Conley Fisheries -->

<!-- Clint Conley, president of Conley fisheries Inc., operates a fleet of 50 cod fishing boats out of Newburyport, Massachusetts. Clint's father started the company 40 years ago but is recently turned the business over to Clint, who has been working for the family business after earning his MBA 10 years ago. Every week day of the year each boat leaves early in the morning, fishes for most of the day, and completes its catch of codfish (3500 pounds) by midafternoon. The boat then has a number of ports where it can sell its daily catch. The price of codfish at some ports is very uncertain and can change quite a bit even on a daily basis. Also, the price of codfish tends to be different at different ports. Furthermore, some ports have only limited demand for codfish, so if a boat arrived relatively later than other fishing boats that port, the catch of fish cannot be sold and so must be disposed of in ocean waters. -->

<!-- To keep Conley fisheries problem simple enough to analyze with ease, assume that the daily operating expenses of the boat are $10,000 per day. Also, assume that the boat is always able to catch all the fish that it can hold (3500 pounds). -->

<!-- Assume that the Conley Fisheries' boat can bring its catch to port in Gloucester or the port in Rockport, Massachusetts.  Gloucester is a major port for codfish with a well-established market. The price of codfish in Gloucester is 3.25 per pound.  This price has been stable for quite some time. The price of codfish in Rockport tends to be a bit higher than  Gloucester but has a lot of variability. Clint has estimated that the daily price of codfish in Rockport is normally distributed with the mean of $\mu$ = 3.65  and a standard deviation of $\sigma$ = .2 per pound. -->

<!-- The port in Gloucester has a very large market for codfish, and so Conley Fisheries never has a problem selling the codfish  there. In contrast the port in Rockport is much smaller and sometimes the boat is unable to sell part or all of its daily catch. Based on past history, Clint has estimated that the demand for codfish that he faces when his boat arrives at port in Rockport obeys the discrete probability distribution depicted below: -->

<!-- ```{r} -->
<!-- data.frame(Demand = c(0, 1000, 2000, 3000, 4000, 5000, 6000), -->
<!--            Probability = c(.02, .03, .05, .08, .33, .29, .22)) -->
<!-- ``` -->

<!-- It is assumed that the price of codfish in Rockport and the demand for codfish in Rockport faced by Conley Fisheries are independent of one another.  Therefore, there is no correlation between the daily price of codfish in the daily demand in Rockport faced by Conley Fisheries. -->

<!-- At the start of any given day, the decision Clint Conley faces is which port to use for selling his daily catch. The price of codfish that the catch might command in Rockport is only known if and when the the boat docks at the port and Clint negotiates with buyers. After the boat has docked at one of the two ports, it must sell its catch at the port or not at all since it takes too much time to pilot the boat out of one port and power it all the way to the other port. -->

<!-- Clint Conley is just as anxious as any other business person to earn a profit. For this reason he wonders if the smart strategy might be to sell his daily catch in Rockport. After all, the expected price of codfish is higher in Rockport, and although the standard deviation of the price is high, and hence there is greater risk with the strategy, Clint is not averse to taking chances when they make good sense. However it also might be true that the smart strategy could be to sell the codfish in Gloucester, since in Gloucester there is ample demand for his daily catch, whereas in Rockport there is the possibility that he might not sell all of his catch (and so potentially lose valuable revenue). It is not clear to him which strategy is best. -->

<!-- One can start to analyze this problem by computing the daily earnings if Clint chooses to sell his daily catch of codfish in Gloucester. The earnings from using Gloucester, denoted by G, is simply: -->

<!--  $$G = 3.25  * 3500 - 10000 = 1375 $$ -->

<!-- which is the revenue of 3.25 dollars per pound times the number of pounds of codfish minus the daily operating expenses of 10,000 dollars. -->

<!-- The computation of daily earnings if Clint chooses Rockport is not so straightforward, because the price and the demand are each uncertain. Therefore the daily earnings from choosing Rockport is an uncertain quantity, i.e., a random variable. In order to make an informed decision as to which port to use, it would be helpful to answer such questions as: -->

<!-- 1. What is the shape of the probability distribution of daily earnings from using Rockport? -->
<!-- 2. On any given day, what is the probability that Conley fisheries would earn more money from using Rockport instead of Gloucester. -->
<!-- 3. On any given day, what is the probability the Conley fisheries will lose money if they use Rockport? -->
<!-- 4. What is the expected daily earnings from using Rockport? -->
<!-- 5. What is the standard deviation of the daily earnings from using Rockport? -->

<!-- The answers to these five questions are, in all likelihood, all that is needed for Clint Conley to choose the port strategy that will best serve the financial interests of Conley fisheries. -->